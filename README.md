# Repository for Diffusion Project
> Hyun Kang, Dohae Lee, Myungjin Shin, In-Kwon Lee

## Implemented Baseline Models
```
{
	'stable_diffusion': StableDiffusionPipeline,
	'composable_diffusion': ComposableStableDiffusionPipeline,
	'structure_diffusion': StructureDiffusionPipeline,
	'attend_and_excite': AttendAndExcitePipeline,
	'syntax_guided_generation': SynGenDiffusionPipeline,
}
```

## Dataset
Dataset should be in '.csv' format in which "prompt" column contains target prompts

## Experiments
Baseline models' default configurations are in "configs/models" file

Configure experiment file (.yaml) to run multiple models all at once

```
python -m run configs/exp/example.yaml
```

## Evaluation
## Prerequisites
- `clean-fid==0.1.35`
- Clone `https://github.com/mjsh34/T2I-CompBench`, install requirements (in a separate environment).

To see all options for evaluation run `python eval.py --help`.

Evaluate single dataset (specified in `./configs/exp/example.yaml`) on `fid`, `blip-vqa`, `clipscore` and `unidet` metrics:
``` sh
python eval.py \
	--config ./configs/exp/example.yaml \
	--methods fid blip-vqa clipscore unidet \
	--t2i_compbench_path /path/to/T2I-CompBench/ \
    --t2i_compbench_pyexe /path/to/T2I-CompBench/venv/bin/python \
	--fid_ref_images_dir /path/to/coco/
```

Evaluate datasets (specified in configs inside `./configs/datasets/`) on `fid`, `blip-vqa`, `clipscore` and `unidet` metrics:
``` sh
python eval.py \
	--config ./configs/datasets/ \
	--methods fid blip-vqa clipscore unidet \
	--t2i_compbench_path /path/to/T2I-CompBench/ \
    --t2i_compbench_pyexe /path/to/T2I-CompBench/venv/bin/python \
	--fid_ref_images_dir /path/to/coco/
```

<!-- # Concept-Diffusion

> Hyun Kang, Dohae Lee, Myungjin Shin, In-Kwon Lee
> 
> Yonsei University
>
> Recent advancements in Text-to-Image (T2I) diffusion models have demonstrated impressive success in generating high-quality images with zero-shot generalization capabilities. Yet, current models struggle to closely adhere to prompt semantics, often misrepresenting or overlooking specific attributes. To address this, we propose a simple, training-free approach that modulates the guidance direction of diffusion models during inference. We first decompose the prompt semantics into a set of concepts, and monitor the guidance trajectory in relation to each concept. Our key observation is that deviations in model's adherence to prompt semantics are highly correlated with divergence of the guidance from one or more of these concepts. Based on this observation, we devise a technique to steer the guidance direction towards any concept from which the model diverges. Extensive experimentation validates that our method improves the semantic alignment of images generated by diffusion models in response to prompts.

<a href="https://arxiv.org/0000.00000"><img src="https://img.shields.io/badge/arXiv-2301.13826-b31b1b.svg" height=20.5></a>
<a href="https://conceptdiffusion.github.io/"><img src="https://img.shields.io/static/v1?label=Project&message=Website&color=red" height=20.5></a> 

<p align="center">
<img src="samples/teaser.png" width="800px"/>  
</p>

## Description
This is the official codebase for **Semantic Guidance Tuning for Text-To-Image Diffusion Models**.

## Setup
### Environment
Following lines setup an environment
```
conda create -n concept_diff --file python=3.9
conda activate concept_diff
```
To install required packages,
```
pip install -r requirements.txt
```

## Usage
To generate an image, you can simply run the `run.py` script with following arguments
```
python -m run <prompt> <subjects> --seed <seed>
```
- _prompt_ (str): prompt text to generate an image e.g., "a red book and a brown dog".
- _subjects_ (str): subjects in the prompt separated by comma (,) e.g., "a red book, a brown dog"
- _seed_ (int \*optional\*): random seed to generate latent noise e.g., "20231204"

Hyperparameters for Concept Diffusion is stored in `configs/default.yaml`
- _concept_guidance_scale_ (float): guidance scale for concept guidance term
- _threshold_\[subject|abstract\]_ (float or "auto"): threshold cosine similarity between main score and concept score for applying concept guidance.
- _upper_bound_ (float): upperbound threshold similarity between main score and concept score for applying concept guidance.  

## Acknowledgement
This code is built upon [diffusers](https://github.com/huggingface/diffusers) and [Perp-Neg](https://github.com/Perp-Neg/Perp-Neg-stablediffusion).

## Citation
If you use this code for your research, please cite the following work: 
 -->
